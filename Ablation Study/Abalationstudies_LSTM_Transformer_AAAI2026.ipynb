{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03af96d6",
   "metadata": {},
   "source": [
    "#### Audio Image - LSTM and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_audio_image.csv\")\n",
    "test = pd.read_csv(\"te_audio_image.csv\")\n",
    "tr_audio_image = train.drop('y',axis=1)\n",
    "te_audio_image = test.drop('y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae259ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518852bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "# Parameters\n",
    "N = 8  # number of features\n",
    "M_TRAIN = X_train.shape[0]  # number of training examples\n",
    "M_TEST = X_test.shape[0]  # number of test examples\n",
    "BATCH = 2  # batch size\n",
    "EPOCH = 10 # number of epochs\n",
    "\n",
    "# Prepare data (assume X_train and X_test are already defined)\n",
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Define a transformer model\n",
    "def create_transformer_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Attention block\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=2)(inputs, inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Skip connection\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Feedforward network\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create the transformer model\n",
    "model = create_transformer_model((1, N))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.5),\n",
    "               keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Binarize predictions\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48313bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_audio_image.csv\")\n",
    "test = pd.read_csv(\"te_audio_image.csv\")\n",
    "tr_audio_image = train.drop('y',axis=1)\n",
    "te_audio_image = test.drop('y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07247a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4473b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f113f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time\n",
    "\n",
    "LAYERS = [8, 16, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train.shape[0]           # number of training examples\n",
    "M_TEST = X_test.shape[0]             # number of test examples\n",
    "N = 8                                # number of features\n",
    "T = 1                                  # Set T to 1 for single timestep per sample\n",
    "BATCH = 2                             # batch size\n",
    "EPOCH = 10                           # number of epochs\n",
    "LR = 0.1                              # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                          # lambda in L2 regularization\n",
    "DP = 0.5                              # dropout rate\n",
    "RDP = 0.0                             # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=LAYERS[0], input_shape=(T, N),\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(3e-2), recurrent_regularizer=l2(3e-2),\n",
    "               dropout=0.0, recurrent_dropout=0.0,\n",
    "               return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[1], return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[2], return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=2, activation='softmax'))  # Adjust for softmax\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=Adam(learning_rate=0.001))  # Adjust learning rate\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,  # Use balanced data and categorical labels\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[ReduceLROnPlateau(monitor='loss', patience=1, factor=0.5), \n",
    "               EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get class predictions\n",
    "# print(classification_report(y_test, y_pred_classes))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert one-hot y_test to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8dd71",
   "metadata": {},
   "source": [
    "#### Text + Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_text_audio.csv\")\n",
    "test = pd.read_csv(\"te_text_audio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bda449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0740c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "# Parameters\n",
    "N = 8  # number of features\n",
    "M_TRAIN = X_train.shape[0]  # number of training examples\n",
    "M_TEST = X_test.shape[0]  # number of test examples\n",
    "BATCH = 2  # batch size\n",
    "EPOCH = 10 # number of epochs\n",
    "\n",
    "# Prepare data (assume X_train and X_test are already defined)\n",
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Define a transformer model\n",
    "def create_transformer_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Attention block\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=2)(inputs, inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Skip connection\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Feedforward network\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create the transformer model\n",
    "model = create_transformer_model((1, N))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.5),\n",
    "               keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Binarize predictions\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00957798",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_text_audio.csv\")\n",
    "test = pd.read_csv(\"tr_text_audio.csv\")\n",
    "tr_audio_image = train.drop('y',axis=1)\n",
    "te_audio_image = test.drop('y',axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n",
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time\n",
    "\n",
    "LAYERS = [8, 16, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train.shape[0]           # number of training examples\n",
    "M_TEST = X_test.shape[0]             # number of test examples\n",
    "N = 8                                # number of features\n",
    "T = 1                                  # Set T to 1 for single timestep per sample\n",
    "BATCH = 2                             # batch size\n",
    "EPOCH = 10                           # number of epochs\n",
    "LR = 0.1                              # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                          # lambda in L2 regularization\n",
    "DP = 0.5                              # dropout rate\n",
    "RDP = 0.0                             # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=LAYERS[0], input_shape=(T, N),\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(3e-2), recurrent_regularizer=l2(3e-2),\n",
    "               dropout=0.0, recurrent_dropout=0.0,\n",
    "               return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[1], return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[2], return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=2, activation='softmax'))  # Adjust for softmax\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=Adam(learning_rate=0.001))  # Adjust learning rate\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,  # Use balanced data and categorical labels\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[ReduceLROnPlateau(monitor='loss', patience=1, factor=0.5), \n",
    "               EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get class predictions\n",
    "# print(classification_report(y_test, y_pred_classes))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert one-hot y_test to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f92820",
   "metadata": {},
   "source": [
    "#### Text + Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Text + Audio\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_text_image.csv\")\n",
    "test = pd.read_csv(\"te_text_image.csv\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "# Parameters\n",
    "N = 8  # number of features\n",
    "M_TRAIN = X_train.shape[0]  # number of training examples\n",
    "M_TEST = X_test.shape[0]  # number of test examples\n",
    "BATCH = 2  # batch size\n",
    "EPOCH = 10 # number of epochs\n",
    "\n",
    "# Prepare data (assume X_train and X_test are already defined)\n",
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Define a transformer model\n",
    "def create_transformer_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Attention block\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=2)(inputs, inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Skip connection\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Feedforward network\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create the transformer model\n",
    "model = create_transformer_model((1, N))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=1, factor=0.5),\n",
    "               keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Binarize predictions\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5f539",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"tr_text_image.csv\")\n",
    "test = pd.read_csv(\"te_text_image.csv\")\n",
    "tr_audio_image = train.drop('y',axis=1)\n",
    "te_audio_image = test.drop('y',axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "new = pd.concat([train,test])\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(new.drop(columns=['y'])),np.array(new['y']))\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n",
    "X_train = X_train.reshape(-1, 1, N)  # Reshape to (samples, timesteps, features)\n",
    "X_test = X_test.reshape(-1, 1, N)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time\n",
    "\n",
    "LAYERS = [8, 16, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train.shape[0]           # number of training examples\n",
    "M_TEST = X_test.shape[0]             # number of test examples\n",
    "N = 8                                # number of features\n",
    "T = 1                                  # Set T to 1 for single timestep per sample\n",
    "BATCH = 2                             # batch size\n",
    "EPOCH = 10                           # number of epochs\n",
    "LR = 0.1                              # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                          # lambda in L2 regularization\n",
    "DP = 0.5                              # dropout rate\n",
    "RDP = 0.0                             # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=LAYERS[0], input_shape=(T, N),\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(3e-2), recurrent_regularizer=l2(3e-2),\n",
    "               dropout=0.0, recurrent_dropout=0.0,\n",
    "               return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[1], return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[2], return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=2, activation='softmax'))  # Adjust for softmax\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "              optimizer=Adam(learning_rate=0.001))  # Adjust learning rate\n",
    "\n",
    "# Train the model\n",
    "start = time()\n",
    "History = model.fit(\n",
    "    X_train, y_train,  # Use balanced data and categorical labels\n",
    "    epochs=EPOCH,\n",
    "    batch_size=BATCH,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[ReduceLROnPlateau(monitor='loss', patience=1, factor=0.5), \n",
    "               EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)]\n",
    ")\n",
    "print(f'Training completed in {time() - start:.2f} secs')\n",
    "\n",
    "# Evaluate\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, batch_size=BATCH)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get class predictions\n",
    "# print(classification_report(y_test, y_pred_classes))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert one-hot y_test to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
